---

title: "Support Vector Machines Project"
data: 2020-02-03
tags: [python,  support vector machine, exploratory data analysis, machine learning, classification, supervised leaning]
header:
excerpt: "Support Vector Machines Project "
mathjax: "true"
---



Welcome to your Support Vector Machine Project! Just follow along with the notebook and instructions below. We will be analyzing the famous iris data set!

## The Data
For this series of lectures, we will be using the famous [Iris flower data set](http://en.wikipedia.org/wiki/Iris_flower_data_set).

The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by Sir Ronald Fisher in the 1936 as an example of discriminant analysis.

The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor), so 150 total samples. Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.

Here's a picture of the three different Iris types:


```python
# The Iris Setosa
from IPython.display import Image
url = 'http://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg'
Image(url,width=300, height=300)
```




![jpeg](/images/output_1_0.jpg)




```python
# The Iris Versicolor
from IPython.display import Image
url = 'http://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg'
Image(url,width=300, height=300)
```




![jpeg](/images/output_2_0.jpg)




```python
# The Iris Virginica
from IPython.display import Image
url = 'http://upload.wikimedia.org/wikipedia/commons/9/9f/Iris_virginica.jpg'
Image(url,width=300, height=300)
```




![jpeg](/images/output_3_0.jpg)



The iris dataset contains measurements for 150 iris flowers from three different species.

The three classes in the Iris dataset:

    Iris-setosa (n=50)
    Iris-versicolor (n=50)
    Iris-virginica (n=50)

The four features of the Iris dataset:

    sepal length in cm
    sepal width in cm
    petal length in cm
    petal width in cm

## Get the data

**Use seaborn to get the iris data by using: iris = sns.load_dataset('iris') **


```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
import pandas as pd
import numpy as np
```

Let's visualize the data and get you started!

## Exploratory Data Analysis

Time to put your data viz skills to the test! Try to recreate the following plots, make sure to import the libraries you'll need!

**Import some libraries you think you'll need.**


```python
from sklearn.datasets import load_iris
iris=load_iris()
iris.keys()
```




    dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])




```python
df_feat = pd.DataFrame(iris['data'],columns=iris['feature_names'])
```


```python
df_feat['types'] = iris['target']
```


```python
df_feat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>types</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>2</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 5 columns</p>
</div>




```python
df_feat['types'].replace([0], 'Setosa', inplace=True)
df_feat['types'].replace([1], 'Versicolor', inplace=True)
df_feat['types'].replace([2], 'Virginica', inplace=True)
```


```python
df_feat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>types</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>Virginica</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>Virginica</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>Virginica</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>Virginica</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>Virginica</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 5 columns</p>
</div>



** Create a pairplot of the data set. Which flower species seems to be the most separable?**

Answer: Setosa


```python
sns.set()
sns.set_style('whitegrid')
sns.pairplot(df_feat,hue='types',palette='Dark2')
```




    <seaborn.axisgrid.PairGrid at 0x24f71359dd8>




![png](/images/output_14_1.png)


**Create a kde plot of sepal_length versus sepal width for setosa species of flower.**


```python
df_feat.plot.kde()
```




    <matplotlib.axes._subplots.AxesSubplot at 0x24f71c28630>




![png](/images/output_16_1.png)



```python
sns.kdeplot(data=setosa['sepal width (cm)'],data2=setosa['sepal length (cm)'],shade=True,cmap='plasma',shade_lowest=False)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x24f749c6a58>




![png](/images/output_17_1.png)



```python
setosa = df_feat[df_feat['types']=='Setosa']
virginica = df_feat[df_feat['types']=='Virginica']
versicolor = df_feat[df_feat['types']=='Versicolor']
ax = sns.kdeplot(setosa['sepal width (cm)'], setosa['sepal length (cm)'],cmap="Reds", shade=True, shade_lowest=False)
#ax = sns.kdeplot(virginica['sepal width (cm)'], virginica['sepal length (cm)'],cmap="Blues", shade=True, shade_lowest=False)
ax = sns.kdeplot(versicolor['sepal width (cm)'], virginica['sepal length (cm)'],cmap="plasma", shade=True, shade_lowest=False)

```


![png](/images/output_18_0.png)



```python
df_feat
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>types</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>Virginica</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>Virginica</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>Virginica</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>Virginica</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>Virginica</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 5 columns</p>
</div>



# Train Test Split

** Split your data into a training set and a testing set.**


```python
from sklearn.model_selection import train_test_split
df_feat.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>types</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Setosa</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_feat2 = df_feat[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',
       'petal width (cm)']]
```


```python
df_feat2
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 4 columns</p>
</div>




```python
X = df_feat2
y = iris['target']
```


```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)
```

# Train a Model

Now its time to train a Support Vector Machine Classifier.

**Call the SVC() model from sklearn and fit the model to the training data.**


```python
from sklearn.svm import SVC
```


```python
model  = SVC()
```


```python
model.fit(X_train,y_train)
```

    C:\Users\User\Anaconda3\lib\site-packages\sklearn\svm\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
      "avoid this warning.", FutureWarning)





    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
        kernel='rbf', max_iter=-1, probability=False, random_state=None,
        shrinking=True, tol=0.001, verbose=False)



## Model Evaluation

**Now get predictions from the model and create a confusion matrix and a classification report.**


```python
pred = model.predict(X_test)
```


```python
from sklearn.metrics import classification_report as cr, confusion_matrix as cm
```


```python
print (cm(y_test,pred))
```

    [[13  0  0]
     [ 0 20  0]
     [ 0  0 12]]



```python
print (cr(y_test,pred))
```

                  precision    recall  f1-score   support

               0       1.00      1.00      1.00        13
               1       1.00      1.00      1.00        20
               2       1.00      1.00      1.00        12

        accuracy                           1.00        45
       macro avg       1.00      1.00      1.00        45
    weighted avg       1.00      1.00      1.00        45



Wow! You should have noticed that your model was pretty good! Let's see if we can tune the parameters to try to get even better (unlikely, and you probably would be satisfied with these results in real like because the data set is quite small, but I just want you to practice using GridSearch.

## Gridsearch Practice

** Import GridsearchCV from SciKit Learn.**


```python
from sklearn.model_selection import GridSearchCV
```

**Create a dictionary called param_grid and fill out some parameters for C and gamma.**


```python
param_grid = {'C' : [0.1,1,10,100,1000],'gamma':[1,0.1,0.01,0.001,0.0001]}
```

** Create a GridSearchCV object and fit it to the training data.**


```python
grid = GridSearchCV(SVC(),param_grid,verbose=3)
grid.fit(X_train,y_train)
```

    C:\Users\User\Anaconda3\lib\site-packages\sklearn\model_selection\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.
      warnings.warn(CV_WARNING, FutureWarning)
    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    [Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
    [Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s


    Fitting 3 folds for each of 25 candidates, totalling 75 fits
    [CV] C=0.1, gamma=1 ..................................................
    [CV] ...................... C=0.1, gamma=1, score=0.917, total=   0.0s
    [CV] C=0.1, gamma=1 ..................................................
    [CV] ...................... C=0.1, gamma=1, score=0.914, total=   0.0s
    [CV] C=0.1, gamma=1 ..................................................
    [CV] ...................... C=0.1, gamma=1, score=0.882, total=   0.1s
    [CV] C=0.1, gamma=0.1 ................................................
    [CV] .................... C=0.1, gamma=0.1, score=0.750, total=   0.0s
    [CV] C=0.1, gamma=0.1 ................................................
    [CV] .................... C=0.1, gamma=0.1, score=0.771, total=   0.0s
    [CV] C=0.1, gamma=0.1 ................................................
    [CV] .................... C=0.1, gamma=0.1, score=0.765, total=   0.0s
    [CV] C=0.1, gamma=0.01 ...............................................
    [CV] ................... C=0.1, gamma=0.01, score=0.722, total=   0.0s
    [CV] C=0.1, gamma=0.01 ...............................................
    [CV] ................... C=0.1, gamma=0.01, score=0.714, total=   0.0s
    [CV] C=0.1, gamma=0.01 ...............................................
    [CV] ................... C=0.1, gamma=0.01, score=0.706, total=   0.0s
    [CV] C=0.1, gamma=0.001 ..............................................
    [CV] .................. C=0.1, gamma=0.001, score=0.361, total=   0.0s
    [CV] C=0.1, gamma=0.001 ..............................................
    [CV] .................. C=0.1, gamma=0.001, score=0.714, total=   0.0s
    [CV] C=0.1, gamma=0.001 ..............................................
    [CV] .................. C=0.1, gamma=0.001, score=0.353, total=   0.0s
    [CV] C=0.1, gamma=0.0001 .............................................
    [CV] ................. C=0.1, gamma=0.0001, score=0.361, total=   0.0s
    [CV] C=0.1, gamma=0.0001 .............................................
    [CV] ................. C=0.1, gamma=0.0001, score=0.714, total=   0.0s
    [CV] C=0.1, gamma=0.0001 .............................................
    [CV] ................. C=0.1, gamma=0.0001, score=0.353, total=   0.0s
    [CV] C=1, gamma=1 ....................................................
    [CV] ........................ C=1, gamma=1, score=0.944, total=   0.0s
    [CV] C=1, gamma=1 ....................................................
    [CV] ........................ C=1, gamma=1, score=0.914, total=   0.0s
    [CV] C=1, gamma=1 ....................................................
    [CV] ........................ C=1, gamma=1, score=0.941, total=   0.0s
    [CV] C=1, gamma=0.1 ..................................................
    [CV] ...................... C=1, gamma=0.1, score=1.000, total=   0.0s
    [CV] C=1, gamma=0.1 ..................................................
    [CV] ...................... C=1, gamma=0.1, score=0.914, total=   0.0s
    [CV] C=1, gamma=0.1 ..................................................
    [CV] ...................... C=1, gamma=0.1, score=0.971, total=   0.0s
    [CV] C=1, gamma=0.01 .................................................
    [CV] ..................... C=1, gamma=0.01, score=0.861, total=   0.0s
    [CV] C=1, gamma=0.01 .................................................
    [CV] ..................... C=1, gamma=0.01, score=0.829, total=   0.0s
    [CV] C=1, gamma=0.01 .................................................
    [CV] ..................... C=1, gamma=0.01, score=0.853, total=   0.0s
    [CV] C=1, gamma=0.001 ................................................
    [CV] .................... C=1, gamma=0.001, score=0.722, total=   0.0s
    [CV] C=1, gamma=0.001 ................................................
    [CV] .................... C=1, gamma=0.001, score=0.714, total=   0.0s
    [CV] C=1, gamma=0.001 ................................................
    [CV] .................... C=1, gamma=0.001, score=0.706, total=   0.0s
    [CV] C=1, gamma=0.0001 ...............................................
    [CV] ................... C=1, gamma=0.0001, score=0.361, total=   0.0s
    [CV] C=1, gamma=0.0001 ...............................................
    [CV] ................... C=1, gamma=0.0001, score=0.714, total=   0.0s
    [CV] C=1, gamma=0.0001 ...............................................
    [CV] ................... C=1, gamma=0.0001, score=0.353, total=   0.0s
    [CV] C=10, gamma=1 ...................................................
    [CV] ....................... C=10, gamma=1, score=0.944, total=   0.0s
    [CV] C=10, gamma=1 ...................................................
    [CV] ....................... C=10, gamma=1, score=0.914, total=   0.0s
    [CV] C=10, gamma=1 ...................................................
    [CV] ....................... C=10, gamma=1, score=0.912, total=   0.0s
    [CV] C=10, gamma=0.1 .................................................
    [CV] ..................... C=10, gamma=0.1, score=1.000, total=   0.0s
    [CV] C=10, gamma=0.1 .................................................
    [CV] ..................... C=10, gamma=0.1, score=0.914, total=   0.0s
    [CV] C=10, gamma=0.1 .................................................
    [CV] ..................... C=10, gamma=0.1, score=0.912, total=   0.0s
    [CV] C=10, gamma=0.01 ................................................
    [CV] .................... C=10, gamma=0.01, score=1.000, total=   0.0s
    [CV] C=10, gamma=0.01 ................................................
    [CV] .................... C=10, gamma=0.01, score=0.943, total=   0.0s
    [CV] C=10, gamma=0.01 ................................................
    [CV] .................... C=10, gamma=0.01, score=0.912, total=   0.0s
    [CV] C=10, gamma=0.001 ...............................................
    [CV] ................... C=10, gamma=0.001, score=0.861, total=   0.0s
    [CV] C=10, gamma=0.001 ...............................................
    [CV] ................... C=10, gamma=0.001, score=0.829, total=   0.0s
    [CV] C=10, gamma=0.001 ...............................................
    [CV] ................... C=10, gamma=0.001, score=0.853, total=   0.0s
    [CV] C=10, gamma=0.0001 ..............................................
    [CV] .................. C=10, gamma=0.0001, score=0.722, total=   0.0s
    [CV] C=10, gamma=0.0001 ..............................................
    [CV] .................. C=10, gamma=0.0001, score=0.714, total=   0.0s
    [CV] C=10, gamma=0.0001 ..............................................
    [CV] .................. C=10, gamma=0.0001, score=0.706, total=   0.0s
    [CV] C=100, gamma=1 ..................................................
    [CV] ...................... C=100, gamma=1, score=0.944, total=   0.0s
    [CV] C=100, gamma=1 ..................................................
    [CV] ...................... C=100, gamma=1, score=0.914, total=   0.0s
    [CV] C=100, gamma=1 ..................................................
    [CV] ...................... C=100, gamma=1, score=0.912, total=   0.0s
    [CV] C=100, gamma=0.1 ................................................
    [CV] .................... C=100, gamma=0.1, score=0.944, total=   0.0s
    [CV] C=100, gamma=0.1 ................................................
    [CV] .................... C=100, gamma=0.1, score=0.914, total=   0.0s
    [CV] C=100, gamma=0.1 ................................................
    [CV] .................... C=100, gamma=0.1, score=0.912, total=   0.0s
    [CV] C=100, gamma=0.01 ...............................................
    [CV] ................... C=100, gamma=0.01, score=1.000, total=   0.0s
    [CV] C=100, gamma=0.01 ...............................................
    [CV] ................... C=100, gamma=0.01, score=0.914, total=   0.0s
    [CV] C=100, gamma=0.01 ...............................................
    [CV] ................... C=100, gamma=0.01, score=0.912, total=   0.0s
    [CV] C=100, gamma=0.001 ..............................................
    [CV] .................. C=100, gamma=0.001, score=1.000, total=   0.0s
    [CV] C=100, gamma=0.001 ..............................................
    [CV] .................. C=100, gamma=0.001, score=0.914, total=   0.0s
    [CV] C=100, gamma=0.001 ..............................................
    [CV] .................. C=100, gamma=0.001, score=0.912, total=   0.2s
    [CV] C=100, gamma=0.0001 .............................................
    [CV] ................. C=100, gamma=0.0001, score=0.861, total=   0.0s
    [CV] C=100, gamma=0.0001 .............................................
    [CV] ................. C=100, gamma=0.0001, score=0.829, total=   0.0s
    [CV] C=100, gamma=0.0001 .............................................
    [CV] ................. C=100, gamma=0.0001, score=0.853, total=   0.0s
    [CV] C=1000, gamma=1 .................................................
    [CV] ..................... C=1000, gamma=1, score=0.944, total=   0.0s
    [CV] C=1000, gamma=1 .................................................
    [CV] ..................... C=1000, gamma=1, score=0.914, total=   0.0s
    [CV] C=1000, gamma=1 .................................................
    [CV] ..................... C=1000, gamma=1, score=0.912, total=   0.0s
    [CV] C=1000, gamma=0.1 ...............................................
    [CV] ................... C=1000, gamma=0.1, score=0.944, total=   0.0s
    [CV] C=1000, gamma=0.1 ...............................................
    [CV] ................... C=1000, gamma=0.1, score=0.914, total=   0.0s
    [CV] C=1000, gamma=0.1 ...............................................
    [CV] ................... C=1000, gamma=0.1, score=0.912, total=   0.0s
    [CV] C=1000, gamma=0.01 ..............................................
    [CV] .................. C=1000, gamma=0.01, score=1.000, total=   0.0s
    [CV] C=1000, gamma=0.01 ..............................................
    [CV] .................. C=1000, gamma=0.01, score=0.914, total=   0.0s
    [CV] C=1000, gamma=0.01 ..............................................
    [CV] .................. C=1000, gamma=0.01, score=0.912, total=   0.0s
    [CV] C=1000, gamma=0.001 .............................................
    [CV] ................. C=1000, gamma=0.001, score=1.000, total=   0.0s
    [CV] C=1000, gamma=0.001 .............................................
    [CV] ................. C=1000, gamma=0.001, score=0.914, total=   0.0s
    [CV] C=1000, gamma=0.001 .............................................
    [CV] ................. C=1000, gamma=0.001, score=0.912, total=   0.0s
    [CV] C=1000, gamma=0.0001 ............................................
    [CV] ................ C=1000, gamma=0.0001, score=1.000, total=   0.0s
    [CV] C=1000, gamma=0.0001 ............................................
    [CV] ................ C=1000, gamma=0.0001, score=0.914, total=   0.0s
    [CV] C=1000, gamma=0.0001 ............................................
    [CV] ................ C=1000, gamma=0.0001, score=0.912, total=   0.0s


    [Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:    0.6s finished
    C:\Users\User\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
      DeprecationWarning)





    GridSearchCV(cv='warn', error_score='raise-deprecating',
                 estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
                               decision_function_shape='ovr', degree=3,
                               gamma='auto_deprecated', kernel='rbf', max_iter=-1,
                               probability=False, random_state=None, shrinking=True,
                               tol=0.001, verbose=False),
                 iid='warn', n_jobs=None,
                 param_grid={'C': [0.1, 1, 10, 100, 1000],
                             'gamma': [1, 0.1, 0.01, 0.001, 0.0001]},
                 pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
                 scoring=None, verbose=3)




```python
grid.best_params_
```




    {'C': 1, 'gamma': 0.1}




```python
grid.best_estimator_
```




    SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)



** Now take that grid model and create some predictions using the test set and create classification reports and confusion matrices for them. Were you able to improve?**


```python
grid.predictions = grid.predict(X_test)
```


```python
print (cm(y_test,grid.predictions))
```

    [[13  0  0]
     [ 0 19  1]
     [ 0  0 12]]



```python
print(cr(y_test,grid.predictions))
```

                  precision    recall  f1-score   support

               0       1.00      1.00      1.00        13
               1       1.00      0.95      0.97        20
               2       0.92      1.00      0.96        12

        accuracy                           0.98        45
       macro avg       0.97      0.98      0.98        45
    weighted avg       0.98      0.98      0.98        45



You should have done about the same or exactly the same, this makes sense, there is basically just one point that is too noisey to grab, which makes sense, we don't want to have an overfit model that would be able to grab that.

## Great Job!
